# ğŸš€ Assignment 2: Debugging Vision Transformers (ViT) with PyCharm & WSL

## ğŸ‘‹ Introduction
This project demonstrates step-by-step debugging and inspection of the Vision Transformer (ViT-B/16) using **PyCharm** + **WSL (Ubuntu 24.04)**. All tensor shapes and values are visualized as the model progresses from raw image input to classification logits.

---

## ğŸ–¥ï¸ Environment
- ğŸ’» **IDE:** PyCharm Professional
- ğŸ§ **Platform:** WSL (Ubuntu 24.04)
- ğŸ¨ **Model:** ViT-B/16  
    - ğŸ§© Patch size: 16  
    - ğŸ§  Embedding dimension: 768  
    - ğŸ”® Attention heads: 12  
    - ğŸ›ï¸ Encoder blocks: 12  
    - ğŸ·ï¸ Classification head for 1000 ImageNet classes

---

## ğŸ–¼ï¸ Input Image & Preprocessing
- ğŸ“· **Selected Image:** `sample.jpg` (personal photo)
- âš¡ **Preprocessing Steps:**
    1. ğŸŒˆ Convert to RGB
    2. âœ‚ï¸ Resize to 224 Ã— 224
    3. âš–ï¸ Normalize with ImageNet means `[0.485, 0.456, 0.406]` & std `[0.229, 0.224, 0.225]`
    4. ğŸ§¬ Convert to tensor `(1, 3, 224, 224)`

---

## ğŸ¦¾ Model Trace: Snapshots

| ğŸªœ **Step**                | ğŸ§© **Shape**       | ğŸ“ **Description**               |
|---------------------------|-------------------|----------------------------------|
| Raw Input                 | `[1, 3, 224, 224]`  | Raw image after preprocessing    |
| ğŸš¦ Patchification         | `[1, 768, 14, 14]`  | Split into 16Ã—16 patches         |
| ğŸ§¹ Flattened Patches      | `[1, 196, 768]`     | 196 patches as 768-dim vectors   |
| ğŸª„ Patch Embeddings       | `[1, 196, 768]`     | Linear projection                |
| ğŸ·ï¸ Class Token            | `[1, 1, 768]`       | Learnable classification token   |
| â• Embeddings + Class      | `[1, 197, 768]`     | Class token prepended            |
| ğŸ”¢ + Positional Encoding  | `[1, 197, 768]`     | Adds spatial info                |
| ğŸ›• Encoder Input           | `[1, 197, 768]`     | Start of first block             |
| ğŸ” Q, K, V                | `[1, 197, 768]`     | Multi-head attention projections |
| ğŸ§® Attention Scores        | `[1, 12, 197, 197]` | Before/after softmax             |
| ğŸ”— Residual + Norm        | `[1, 197, 768]`     | Output of attention block        |
| ğŸ§‘â€ğŸ’» MLP Hidden            | `[1, 197, 3072]`    | Feed-forward network output      |
| ğŸ Encoder Output          | `[1, 197, 768]`     | After 1/2/12 blocks              |
| ğŸ·ï¸ Class Token            | `[1, 768]`          | Extracted for classification     |
| ğŸš¨ Logits                 | `[1, 1000]`         | Projected to ImageNet classes    |
| ğŸ§® Softmax                | `[1, 1000]`         | Probability for each class       |

---

## â“ Key Questions

- **ğŸ§© Why split images into patches?**  
  > Transformers require sequences. Patches convert 2D images to 1D token sequences.

- **ğŸ·ï¸ Why add a class token?**  
  > Aggregates patch info. Classification uses its representation.

- **ğŸ”¢ Why positional encodings?**  
  > Provide spatial info since transformers are permutation-invariant.

- **ğŸ” Why Q, K, V with same dimensions?**  
  > Allows matrix operations for attention, compatible with number of tokens.

- **ğŸ”— Why do residuals preserve shape?**  
  > Stabilizes training for deep stacks.

- **ğŸ·ï¸ Why only the class token for classification?**  
  > Serves as summary of all patches for final prediction.

---

## ğŸŒˆ Reflection
Debugging step-by-step with PyCharm truly unlocked my understanding of the Vision Transformer architecture â€” seeing how data flows, how attention works, and appreciating the design choices (patches, class token, positional encoding).

---

## ğŸƒâ€â™€ï¸ How to Run

1. Open project in **PyCharm**.
2. Choose WSL as the Python interpreter.
3. Put `sample.jpg` in the project folder.
4. Run or debug **`main3.py`**.

---

## ğŸ—‚ï¸ File Structure

Assignment_2/
â”œâ”€â”€ main3.py # Main ViT debugging script
â”œâ”€â”€ sample.jpg # Input image for testing
â””â”€â”€ README.md # Project documentation


---

## ğŸ“š License

This project is part of a university assignment. For educational use only.
